#!/bin/bash

# Chaos Engineering Tests for Yeelo Platform
# Learning Notes:
# - Tests system resilience under failure conditions
# - Validates auto-scaling and recovery mechanisms
# - Simulates real-world failure scenarios
# - Includes automated rollback and recovery validation

set -e

NAMESPACE="yeelo-platform"
CHAOS_DURATION="300s"  # 5 minutes
RECOVERY_TIMEOUT="600s"  # 10 minutes

echo "ðŸ”¥ Starting Chaos Engineering Tests for Yeelo Platform..."

# Function to check service health
check_service_health() {
    local service=$1
    local expected_replicas=${2:-1}
    
    echo "ðŸ” Checking health of $service..."
    
    local ready_replicas=$(kubectl get deployment $service -n $NAMESPACE -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
    
    if [ "$ready_replicas" -ge "$expected_replicas" ]; then
        echo "âœ… $service is healthy ($ready_replicas replicas ready)"
        return 0
    else
        echo "âŒ $service is unhealthy ($ready_replicas replicas ready, expected $expected_replicas)"
        return 1
    fi
}

# Function to run load test during chaos
run_background_load() {
    echo "ðŸš€ Starting background load test..."
    
    k6 run --duration 10m --vus 100 k6/campaign-spike.js \
        --env BASE_URL=https://api-staging.yeelo.com \
        --quiet &
    
    LOAD_TEST_PID=$!
    echo "ðŸ“Š Load test running with PID: $LOAD_TEST_PID"
}

# Function to stop background load
stop_background_load() {
    if [ ! -z "$LOAD_TEST_PID" ]; then
        echo "ðŸ›‘ Stopping background load test..."
        kill $LOAD_TEST_PID 2>/dev/null || true
        wait $LOAD_TEST_PID 2>/dev/null || true
    fi
}

# Function to wait for recovery
wait_for_recovery() {
    local service=$1
    local expected_replicas=${2:-2}
    local timeout=${3:-600}
    
    echo "â³ Waiting for $service to recover (timeout: ${timeout}s)..."
    
    local start_time=$(date +%s)
    while true; do
        local current_time=$(date +%s)
        local elapsed=$((current_time - start_time))
        
        if [ $elapsed -gt $timeout ]; then
            echo "âŒ Recovery timeout for $service"
            return 1
        fi
        
        if check_service_health $service $expected_replicas; then
            echo "âœ… $service recovered in ${elapsed}s"
            return 0
        fi
        
        sleep 10
    done
}

# Trap to ensure cleanup
trap 'stop_background_load; echo "ðŸ§¹ Cleanup completed"' EXIT

# Pre-chaos health check
echo "ðŸ¥ Pre-chaos health check..."
services=("api-gateway" "auth-service" "orders-service" "campaigns-service" "ai-service" "worker-golang")

for service in "${services[@]}"; do
    if ! check_service_health $service 2; then
        echo "âŒ Pre-chaos health check failed for $service"
        exit 1
    fi
done

echo "âœ… All services healthy before chaos testing"

# Start background load
run_background_load

# Chaos Test 1: Pod Termination
echo ""
echo "ðŸ”¥ Chaos Test 1: Random Pod Termination"
echo "Randomly terminating pods to test resilience..."

for i in {1..5}; do
    # Get random pod from a random service
    service=${services[$RANDOM % ${#services[@]}]}
    pod=$(kubectl get pods -n $NAMESPACE -l app=$service -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
    
    if [ ! -z "$pod" ]; then
        echo "ðŸ’¥ Terminating pod: $pod"
        kubectl delete pod $pod -n $NAMESPACE --grace-period=0 --force
        sleep 30
    fi
done

# Wait for recovery
echo "â³ Waiting for pod termination recovery..."
sleep 60

for service in "${services[@]}"; do
    wait_for_recovery $service 2 300
done

# Chaos Test 2: Node Drain Simulation
echo ""
echo "ðŸ”¥ Chaos Test 2: Node Drain Simulation"
echo "Cordoning and draining a node to test rescheduling..."

# Get a worker node (not master)
worker_node=$(kubectl get nodes --no-headers | grep -v master | grep Ready | head -n 1 | awk '{print $1}')

if [ ! -z "$worker_node" ]; then
    echo "ðŸš§ Cordoning node: $worker_node"
    kubectl cordon $worker_node
    
    echo "ðŸŒŠ Draining node: $worker_node"
    kubectl drain $worker_node --ignore-daemonsets --delete-emptydir-data --force --grace-period=30 &
    DRAIN_PID=$!
    
    # Let it drain for a bit, then uncordon
    sleep 120
    
    echo "ðŸ”“ Uncordoning node: $worker_node"
    kubectl uncordon $worker_node
    
    # Kill the drain process if still running
    kill $DRAIN_PID 2>/dev/null || true
    
    # Wait for recovery
    for service in "${services[@]}"; do
        wait_for_recovery $service 2 300
    done
fi

# Chaos Test 3: Network Partition Simulation
echo ""
echo "ðŸ”¥ Chaos Test 3: Network Partition Simulation"
echo "Simulating network issues with network policies..."

# Create restrictive network policy
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: chaos-network-policy
  namespace: $NAMESPACE
spec:
  podSelector:
    matchLabels:
      app: orders-service
  policyTypes:
  - Ingress
  - Egress
  ingress: []  # Block all ingress
  egress:
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
EOF

echo "ðŸŒ Network partition applied to orders-service"
sleep 60

# Remove the restrictive policy
kubectl delete networkpolicy chaos-network-policy -n $NAMESPACE

echo "ðŸ”— Network partition removed"

# Wait for recovery
wait_for_recovery "orders-service" 2 300

# Chaos Test 4: Resource Exhaustion
echo ""
echo "ðŸ”¥ Chaos Test 4: Resource Exhaustion Simulation"
echo "Creating resource-hungry pods to test resource limits..."

# Create a resource-hungry deployment
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resource-hog
  namespace: $NAMESPACE
spec:
  replicas: 3
  selector:
    matchLabels:
      app: resource-hog
  template:
    metadata:
      labels:
        app: resource-hog
    spec:
      containers:
      - name: cpu-hog
        image: busybox
        command: ["sh", "-c", "while true; do echo 'consuming CPU'; done"]
        resources:
          requests:
            cpu: 500m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 512Mi
EOF

echo "ðŸ· Resource hog deployed"
sleep 120

# Check if services are still healthy under resource pressure
echo "ðŸ” Checking service health under resource pressure..."
for service in "${services[@]}"; do
    check_service_health $service 1  # Lower expectation under pressure
done

# Clean up resource hog
kubectl delete deployment resource-hog -n $NAMESPACE

echo "ðŸ§¹ Resource hog cleaned up"

# Chaos Test 5: Database Connection Chaos
echo ""
echo "ðŸ”¥ Chaos Test 5: Database Connection Chaos"
echo "Simulating database connectivity issues..."

# Scale down database connections by restarting services
echo "ðŸ”„ Restarting database-dependent services..."
kubectl rollout restart deployment/auth-service -n $NAMESPACE
kubectl rollout restart deployment/orders-service -n $NAMESPACE

# Wait for rollout to complete
kubectl rollout status deployment/auth-service -n $NAMESPACE --timeout=300s
kubectl rollout status deployment/orders-service -n $NAMESPACE --timeout=300s

# Verify recovery
for service in "auth-service" "orders-service"; do
    wait_for_recovery $service 2 300
done

# Final health check
echo ""
echo "ðŸ¥ Final health check after all chaos tests..."
all_healthy=true

for service in "${services[@]}"; do
    if ! check_service_health $service 2; then
        all_healthy=false
    fi
done

# Stop background load
stop_background_load

# Results
echo ""
echo "ðŸ“Š Chaos Engineering Test Results:"
echo "=================================="

if [ "$all_healthy" = true ]; then
    echo "âœ… All chaos tests passed! System demonstrated excellent resilience."
    echo "ðŸŽ¯ Key observations:"
    echo "   - Auto-scaling responded appropriately to failures"
    echo "   - Services recovered within acceptable timeframes"
    echo "   - No data loss or corruption detected"
    echo "   - Load balancing handled traffic redistribution well"
else
    echo "âŒ Some chaos tests revealed issues that need attention."
    echo "ðŸ” Recommended actions:"
    echo "   - Review service health checks and readiness probes"
    echo "   - Adjust resource limits and requests"
    echo "   - Improve error handling and circuit breaker patterns"
    echo "   - Consider implementing more aggressive auto-scaling policies"
fi

echo ""
echo "ðŸ“ˆ Monitor the following dashboards for detailed analysis:"
echo "   - Grafana: kubectl port-forward svc/prometheus-grafana 3000:80 -n monitoring"
echo "   - Prometheus: kubectl port-forward svc/prometheus-kube-prometheus-prometheus 9090:9090 -n monitoring"
echo "   - Jaeger: kubectl port-forward svc/jaeger-query 16686:16686 -n monitoring"

echo ""
echo "ðŸ”¥ Chaos engineering tests completed!"
